{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import functools\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import yaml\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageColor\n",
    "from PIL import ImageFilter\n",
    "from scipy.stats import norm\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import affinity\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './evaluation-dataset/site/synthetic-0.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-213cf855ca5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_xml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounding_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m     \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m     \u001b[0mimage_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdest_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda3/envs/carnd-capstone/lib/python2.7/xml/etree/ElementTree.pyc\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, file_or_filename, encoding, xml_declaration, default_namespace, method)\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_or_filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_or_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m         \u001b[0mwrite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './evaluation-dataset/site/synthetic-0.xml'"
     ]
    }
   ],
   "source": [
    "sim_backgrounds = [\n",
    "    'synthetic-data/backgrounds/sim/bg-1.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-2.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-3.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-4.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-5.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-6.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-7.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-8.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-9.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-10.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-11.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-12.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-13.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-14.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-15.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-16.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-17.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-18.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-19.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-20.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-21.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-22.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-23.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-24.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-25.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-26.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-27.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-28.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-29.jpg',\n",
    "    'synthetic-data/backgrounds/sim/bg-30.jpg'\n",
    "]\n",
    "sim_red = [\n",
    "    'synthetic-data/elements/sim/red-large.jpg',\n",
    "    'synthetic-data/elements/sim/red-medium.jpg',\n",
    "    'synthetic-data/elements/sim/red-small.jpg'\n",
    "]\n",
    "sim_green = [\n",
    "    'synthetic-data/elements/sim/green-large.jpg',\n",
    "    'synthetic-data/elements/sim/green-medium.jpg',\n",
    "    'synthetic-data/elements/sim/green-small.jpg'\n",
    "]\n",
    "sim_yellow = [\n",
    "    'synthetic-data/elements/sim/yellow-large.jpg',\n",
    "    'synthetic-data/elements/sim/yellow-medium.jpg',\n",
    "    'synthetic-data/elements/sim/yellow-small.jpg'\n",
    "]\n",
    "\n",
    "site_backgrounds = [\n",
    "    'synthetic-data/backgrounds/site/bg-1.jpg',\n",
    "    'synthetic-data/backgrounds/site/bg-2.jpg',\n",
    "    'synthetic-data/backgrounds/site/bg-3.jpg',\n",
    "    'synthetic-data/backgrounds/site/bg-4.jpg',\n",
    "    'synthetic-data/backgrounds/site/bg-5.jpg',\n",
    "    'synthetic-data/backgrounds/site/bg-6.jpg',\n",
    "    'synthetic-data/backgrounds/site/bg-7.jpg',\n",
    "    'synthetic-data/backgrounds/site/bg-8.jpg',\n",
    "    'synthetic-data/backgrounds/site/bg-9.jpg',\n",
    "    'synthetic-data/backgrounds/site/bg-10.jpg',\n",
    "    'synthetic-data/backgrounds/site/bg-11.jpg',\n",
    "    'synthetic-data/backgrounds/site/bg-12.jpg'\n",
    "]\n",
    "site_red = [\n",
    "    'synthetic-data/elements/site/red-1.jpg',\n",
    "    'synthetic-data/elements/site/red-2.jpg',\n",
    "    'synthetic-data/elements/site/red-3.jpg',\n",
    "    'synthetic-data/elements/site/red-4.jpg',\n",
    "    'synthetic-data/elements/site/red-5.jpg',\n",
    "    'synthetic-data/elements/site/red-6.jpg',\n",
    "    'synthetic-data/elements/site/red-7.jpg',\n",
    "    'synthetic-data/elements/site/red-8.jpg',\n",
    "    'synthetic-data/elements/site/red-9.jpg',\n",
    "    'synthetic-data/elements/site/red-10.jpg',\n",
    "    'synthetic-data/elements/site/red-11.jpg',\n",
    "    'synthetic-data/elements/site/red-12.jpg',\n",
    "    'synthetic-data/elements/site/red-13.jpg',\n",
    "    'synthetic-data/elements/site/red-14.jpg',\n",
    "]\n",
    "site_green = [\n",
    "    'synthetic-data/elements/site/green-1.jpg',\n",
    "    'synthetic-data/elements/site/green-2.jpg',\n",
    "    'synthetic-data/elements/site/green-3.jpg',\n",
    "    'synthetic-data/elements/site/green-4.jpg',\n",
    "    'synthetic-data/elements/site/green-5.jpg',\n",
    "    'synthetic-data/elements/site/green-6.jpg',\n",
    "    'synthetic-data/elements/site/green-7.jpg',\n",
    "    'synthetic-data/elements/site/green-8.jpg',\n",
    "    'synthetic-data/elements/site/green-9.jpg',\n",
    "    'synthetic-data/elements/site/green-10.jpg',\n",
    "    'synthetic-data/elements/site/green-11.jpg',\n",
    "    'synthetic-data/elements/site/green-12.jpg',\n",
    "]\n",
    "site_yellow = [\n",
    "    'synthetic-data/elements/site/yellow-1.jpg',\n",
    "    'synthetic-data/elements/site/yellow-2.jpg',\n",
    "    'synthetic-data/elements/site/yellow-3.jpg',\n",
    "    'synthetic-data/elements/site/yellow-4.jpg',\n",
    "    'synthetic-data/elements/site/yellow-5.jpg',\n",
    "    'synthetic-data/elements/site/yellow-6.jpg',\n",
    "    'synthetic-data/elements/site/yellow-7.jpg',\n",
    "    'synthetic-data/elements/site/yellow-8.jpg',\n",
    "    'synthetic-data/elements/site/yellow-9.jpg',\n",
    "    'synthetic-data/elements/site/yellow-10.jpg',\n",
    "    'synthetic-data/elements/site/yellow-11.jpg',\n",
    "]\n",
    "\n",
    "def adjust_gamma(image, gamma=1.0):\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([\n",
    "      ((i / 255.0) ** invGamma) * 255\n",
    "      for i in np.arange(0, 256)])\n",
    "    return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))\n",
    "\n",
    "def adjust_gamma(img, gamma=1.0):\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([\n",
    "      ((i / 255.0) ** inv_gamma) * 255\n",
    "      for i in np.arange(0, 256)])\n",
    "    return cv2.LUT(img.astype(np.uint8), table.astype(np.uint8))\n",
    "\n",
    "def adjust_contrast(img):\n",
    "    clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(2,2))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    img[:,:,0] = clahe.apply(img[:,:,0])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n",
    "    return img\n",
    "\n",
    "def preprocess(img, gamma):\n",
    "    img = adjust_contrast(img)\n",
    "    img = adjust_gamma(img, gamma)\n",
    "    return img\n",
    "\n",
    "def create_object(wrapper, label, bounding_box):\n",
    "    obj = ET.SubElement(wrapper, 'object')\n",
    "\n",
    "    name = ET.SubElement(obj, 'name')\n",
    "    name.text = label\n",
    "    \n",
    "    pose = ET.SubElement(obj, 'pose')\n",
    "    pose.text = 'Unspecified'\n",
    "    \n",
    "    truncated = ET.SubElement(obj, 'truncated')\n",
    "    truncated.text = str(0)\n",
    "    \n",
    "    difficult = ET.SubElement(obj, 'difficult')\n",
    "    difficult.text = str(0)\n",
    "\n",
    "    bndbox = ET.SubElement(obj, 'bndbox')\n",
    "\n",
    "    xmin = ET.SubElement(bndbox, 'xmin')\n",
    "    xmin.text = str(bounding_box[0])\n",
    "\n",
    "    ymin = ET.SubElement(bndbox, 'ymin')\n",
    "    ymin.text = str(bounding_box[1])\n",
    "\n",
    "    xmax = ET.SubElement(bndbox, 'xmax')\n",
    "    xmax.text = str(bounding_box[2])\n",
    "\n",
    "    ymax = ET.SubElement(bndbox, 'ymax')\n",
    "    ymax.text = str(bounding_box[3])\n",
    "\n",
    "def create_xml(name, width, height, bounding_boxes, labels):\n",
    "    annotation = ET.Element('annotation')\n",
    "    \n",
    "    filename = ET.SubElement(annotation, 'filename')\n",
    "    filename.text = name\n",
    "    \n",
    "    size = ET.SubElement(annotation, 'size')\n",
    "\n",
    "    w = ET.SubElement(size, 'width')\n",
    "    w.text = str(width)\n",
    "\n",
    "    h = ET.SubElement(size, 'height')\n",
    "    h.text = str(height)\n",
    "\n",
    "    depth = ET.SubElement(size, 'depth')\n",
    "    depth.text = str(3)\n",
    "    \n",
    "    segmented = ET.SubElement(annotation, 'segmented')\n",
    "    segmented.text = str(0)\n",
    "    \n",
    "    for i in range(len(bounding_boxes)):\n",
    "        create_object(annotation, labels[i], bounding_boxes[i])\n",
    "    return annotation\n",
    "    \n",
    "def find_overlapping_bounding_box(bounding_boxes, new_bounding_box):\n",
    "    max_intersection = 0\n",
    "    for i in range(len(bounding_boxes)):\n",
    "        min_x, min_y, max_x, max_y = bounding_boxes[i]\n",
    "        new_min_x, new_min_y, new_max_x, new_max_y = new_bounding_box\n",
    "#         print(new_bounding_box)\n",
    "        existing_bb = Polygon([\n",
    "            (min_x, min_y),\n",
    "            (max_x, min_y),\n",
    "            (max_x, max_y),\n",
    "            (min_x, max_y)\n",
    "        ])\n",
    "        new_bb= Polygon([\n",
    "            (new_min_x, new_min_y),\n",
    "            (new_max_x, new_min_y),\n",
    "            (new_max_x, new_max_y),\n",
    "            (new_min_x, new_max_y)\n",
    "        ])\n",
    "        intersection = existing_bb.intersection(new_bb)\n",
    "        area_of_overlap = intersection.area / new_bb.area if new_bb.area < existing_bb.area else intersection.area / existing_bb.area\n",
    "\n",
    "        if area_of_overlap > max_intersection:\n",
    "            max_intersection = area_of_overlap\n",
    "    return max_intersection\n",
    "        \n",
    "def rotate_bb(bb, degrees):\n",
    "    min_x, min_y, max_x, max_y = bb\n",
    "    poly = Polygon([\n",
    "        (min_x, min_y),\n",
    "        (max_x, min_y),\n",
    "        (max_x, max_y),\n",
    "        (min_x, max_y)\n",
    "    ])\n",
    "    modified = affinity.rotate(poly, degrees)\n",
    "    min_x, min_y, max_x, max_y = modified.bounds\n",
    "    return [int(min_x), int(max_y), int(max_x), int(min_y)]\n",
    "    \n",
    "def rescale_bb(bb, ratio):\n",
    "    min_x, min_y, max_x, max_y = bb\n",
    "    poly = Polygon([\n",
    "        (min_x, min_y),\n",
    "        (max_x, min_y),\n",
    "        (max_x, max_y),\n",
    "        (min_x, max_y)\n",
    "    ])\n",
    "    modified = affinity.scale(poly, xfact=ratio, yfact=ratio)\n",
    "    \n",
    "    min_x, min_y, max_x, max_y = modified.bounds\n",
    "    return [int(min_x), int(max_y), int(max_x), int(min_y)]\n",
    "\n",
    "def add_to_image(bg, img_set, num, other_bounding_boxes):\n",
    "    bounding_boxes = []\n",
    "    for num_light in range(random.randint(1, num)):\n",
    "        # 0 = large, 1 = medium, 2 = small\n",
    "        img_num = random.randint(0, len(img_set) - 1)\n",
    "        synth_image = Image.open(img_set[img_num]).convert('RGBA')\n",
    "        \n",
    "        # 50 percent chance of flipping the image horizontally\n",
    "        flip_chance = random.uniform(0.0, 1.0)\n",
    "        if flip_chance > 0.5:\n",
    "            synth_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            \n",
    "        # get the element dimensions\n",
    "        light_width, light_height = synth_image.size\n",
    "        bg_width, bg_height = bg.size\n",
    "\n",
    "        # random rotation/rescale rtios\n",
    "        degrees = random.uniform(-3.0, 3.0)\n",
    "        ratio = random.uniform(0.9, 1.8)\n",
    "#         if img_num == 0:\n",
    "#             # large images\n",
    "#             ratio = random.uniform(0.5, 1.5)\n",
    "#         elif img_num == 1:\n",
    "#             # medium images\n",
    "#             ratio = random.uniform(0.5, 1.5)\n",
    "#         else:\n",
    "#             # small images\n",
    "#             ratio = random.uniform(0.7, 1.0)\n",
    "\n",
    "        while True:\n",
    "            upper_left_x = random.randint(-light_width / 2, bg_width - light_width / 2)\n",
    "            upper_left_y = random.randint(-light_width / 2, bg_height + light_height / 2 - float(bg_height) * 0.4)\n",
    "\n",
    "            bounding_box = rescale_bb(rotate_bb([\n",
    "                upper_left_x,\n",
    "                upper_left_y + light_height,\n",
    "                upper_left_x + light_width,\n",
    "                upper_left_y\n",
    "            ], degrees), ratio)\n",
    "\n",
    "            overlap = find_overlapping_bounding_box(bounding_boxes + other_bounding_boxes, bounding_box)\n",
    "            # check to make sure this isn't osverlapping an existing image, and isn't over 50% off screen\n",
    "            if overlap < 0.15 and bounding_box[3] < bg.size[1] - light_height / 2 and bounding_box[2] < bg.size[0] - light_width / 2:\n",
    "                break\n",
    "        \n",
    "        # apply random rotation\n",
    "        rotated_image = synth_image.rotate(degrees, expand=1)\n",
    "        # apply random resizing\n",
    "        rescaled_image = rotated_image.resize(\n",
    "            (int(rotated_image.width * ratio), int(rotated_image.height * ratio)),\n",
    "            resample=Image.BICUBIC\n",
    "        )\n",
    "        final_image = rescaled_image\n",
    "        final_light_width, final_light_height = final_image.size\n",
    "        # apply random gaussian blur to larger images\n",
    "#         print('final_light_height', final_light_height)\n",
    "        if final_light_height > 150:\n",
    "            final_image = rescaled_image.filter(ImageFilter.GaussianBlur(radius=random.randint(0,1))) \n",
    "        bg.paste(\n",
    "            final_image,\n",
    "            (bounding_box[0], bounding_box[3]),\n",
    "            final_image\n",
    "        )\n",
    "\n",
    "        # bounding box is in the format xmin, ymin, xmax, ymax\n",
    "        bounding_boxes.append(bounding_box)\n",
    "    return bounding_boxes\n",
    "\n",
    "# at most we want the light half-obscured\n",
    "# currently this outputs images and labels to a directory called output/ (model-playground/output/)\n",
    "all_images = []\n",
    "all_labels = []\n",
    "entries = []\n",
    "dest_folder = './evaluation-dataset/site/'\n",
    "yaml_path = './evaluation-dataset/site/images.yaml'\n",
    "backgrounds = site_backgrounds\n",
    "green = site_green\n",
    "yellow = site_yellow\n",
    "red = site_red\n",
    "\n",
    "num_backgrounds = len(backgrounds)\n",
    "\n",
    "for i in range(2000):\n",
    "    bg = Image.open(backgrounds[random.randint(0, num_backgrounds - 1)])\n",
    "\n",
    "    filename = 'synthetic-' + str(i)\n",
    "    bounding_boxes = []\n",
    "    labels = []\n",
    "\n",
    "    new_bounding_boxes = add_to_image(bg, green, 3, bounding_boxes)\n",
    "    for i in range(len(new_bounding_boxes)):\n",
    "        labels.append('Green_light')\n",
    "        bounding_boxes.append(new_bounding_boxes[i])\n",
    "\n",
    "    new_bounding_boxes = add_to_image(bg, yellow, 3, bounding_boxes)\n",
    "    for i in range(len(new_bounding_boxes)):\n",
    "        labels.append('Yellow_light')\n",
    "        bounding_boxes.append(new_bounding_boxes[i])\n",
    "\n",
    "    new_bounding_boxes = add_to_image(bg, red, 3, bounding_boxes)\n",
    "    for i in range(len(new_bounding_boxes)):\n",
    "        labels.append('Red_light')\n",
    "        bounding_boxes.append(new_bounding_boxes[i])\n",
    "\n",
    "    bg_width, bg_height = bg.size\n",
    "    \n",
    "    annotation = create_xml(filename + '.jpg', bg_width, bg_height, bounding_boxes, labels)\n",
    "    ET.ElementTree(annotation).write(dest_folder + filename + '.xml')\n",
    "    image_filename = dest_folder + filename + '.jpg'\n",
    "    \n",
    "    bg = Image.fromarray(preprocess(np.asarray(bg), random.uniform(0.4, 0.6)))\n",
    "    bg.save(image_filename)\n",
    "    \n",
    "    # everything below is for writing yaml file for the tfrecord conversion\n",
    "    all_labels.append([bounding_boxes, labels])\n",
    "    entry = {\n",
    "        'path': '',\n",
    "        'boxes': []\n",
    "    }\n",
    "    entry['path'] = image_filename\n",
    "    for i in range(len(bounding_boxes)):\n",
    "        x_min, y_min, x_max, y_max = bounding_boxes[i]\n",
    "        entry['boxes'].append({\n",
    "            'label': labels[i],\n",
    "            'x_min': x_min,\n",
    "            'x_max': x_max,\n",
    "            # reverse y min and max because the Y axis flips\n",
    "            'y_min': y_max,\n",
    "            'y_max': y_min,\n",
    "        })\n",
    "    \n",
    "    entries.append(entry)\n",
    "\n",
    "with open(yaml_path, 'w') as file:\n",
    "    yaml.dump(entries, file)\n",
    "\n",
    "print('Done!')\n",
    "#     plt.imshow(bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of content of existing database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryGreen = \"./alex-data/simulator_dataset_rgb/Green/labels/\"\n",
    "directoryYellow = \"./alex-data/simulator_dataset_rgb/Yellow/labels/\"\n",
    "directoryRed = \"./alex-data/simulator_dataset_rgb/Red/labels/\"\n",
    "\n",
    "directory = directoryRed\n",
    "\n",
    "with open(\"evaluation_simulator_training_dataset.csv\", \"a\") as myfile:\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            root = ET.parse(directory+filename).getroot()\n",
    "            for obj in root.findall('object'):\n",
    "                myfile.write(obj.find('name').text + \",\")\n",
    "                bbox = obj.find('bndbox')\n",
    "                myfile.write(bbox.find('xmin').text + \",\")\n",
    "                myfile.write(bbox.find('xmax').text + \",\")\n",
    "                myfile.write(bbox.find('ymin').text + \",\")\n",
    "                myfile.write(bbox.find('ymax').text)\n",
    "                myfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow' from '/Users/gleb/miniconda3/envs/carnd-capstone/lib/python2.7/site-packages/tensorflow/__init__.pyc'>\n",
      "Reading yaml...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gleb/miniconda3/envs/carnd-capstone/lib/python2.7/site-packages/ipykernel_launcher.py:64: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 examples...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "INPUT_YAML = \"./evaluation-dataset/site/images.yaml\"\n",
    "TFRECORD_DESTINATION = './evaluation-dataset/site/images.record'\n",
    "\n",
    "LABEL_DICT =  {\n",
    "    \"Green_light\" : 1,\n",
    "    \"Red_light\" : 2,\n",
    "    \"Yellow_light\" : 3,\n",
    "}\n",
    "\n",
    "def create_tf_record(example):\n",
    "\n",
    "    height = 600 # Image height\n",
    "    width = 800 # Image width\n",
    "\n",
    "    filename = example['path'] # Filename of the image. Empty if image is not from file\n",
    "    filename = filename.encode()\n",
    "\n",
    "    with tf.gfile.GFile(example['path'], 'rb') as fid:\n",
    "        encoded_image = fid.read()\n",
    "\n",
    "    image_format = 'jpg'.encode()\n",
    "\n",
    "    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n",
    "    xmaxs = [] # List of normalized right x coordinates in bounding box\n",
    "                # (1 per box)\n",
    "    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n",
    "    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n",
    "                # (1 per box)\n",
    "    classes_text = [] # List of string class name of bounding box (1 per box)\n",
    "    classes = [] # List of integer class id of bounding box (1 per box)\n",
    "\n",
    "    for box in example['boxes']:\n",
    "        #if box['occluded'] is False:\n",
    "        #print(\"adding box\")\n",
    "        xmins.append(float(float(box['x_min']) / float(width)))\n",
    "        xmaxs.append(float(float(box['x_max']) / float(width)))\n",
    "        ymins.append(float(float(box['y_min']) / float(height)))\n",
    "        ymaxs.append(float(float(box['y_max']) / float(height)))\n",
    "        classes_text.append(box['label'].encode())\n",
    "        classes.append(int(LABEL_DICT[box['label']]))\n",
    "\n",
    "\n",
    "    tf_record = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_image),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "\n",
    "    return tf_record\n",
    "\n",
    "print(tf)\n",
    "writer = tf.python_io.TFRecordWriter(TFRECORD_DESTINATION)\n",
    "\n",
    "print(\"Reading yaml...\")\n",
    "examples = yaml.load(open(INPUT_YAML, 'rb').read())\n",
    "\n",
    "len_examples = len(examples)\n",
    "print(\"Loaded \" + str(len(examples)) + \" examples...\")\n",
    "\n",
    "for i in range(2000):\n",
    "    examples[i]['path'] = os.path.abspath(examples[i]['path'])\n",
    "    example = examples[i]\n",
    "    \n",
    "#     print(example)\n",
    "    tf_record = create_tf_record(example)\n",
    "    writer.write(tf_record.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test detection code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# the following function is to add data in a dictionary in order to analyse the detection boxes based on the bbox size\n",
    "INDEX_DETECT_CORRECTLY = 0\n",
    "INDEX_DETECT_INCORRECTLY = 1\n",
    "INDEX_NOT_DETECTED = 2\n",
    "INDEX_TOTAL_SAMPLE = 3\n",
    "\n",
    "def add_bbox_sample(dictionary, bbox_size, light_color_id, is_detected, is_detected_correctly):\n",
    "    if not ((bbox_size,light_color_id) in dictionary):\n",
    "        dictionary.update({(bbox_size,light_color_id):[0,0,0,0]})\n",
    "    #update\n",
    "    if is_detected:\n",
    "        if is_detected_correctly:\n",
    "            dictionary[(bbox_size,light_color_id)][INDEX_DETECT_CORRECTLY] += 1\n",
    "        else:\n",
    "            dictionary[(bbox_size,light_color_id)][INDEX_DETECT_INCORRECTLY] += 1\n",
    "    else:\n",
    "        dictionary[(bbox_size,light_color_id)][INDEX_NOT_DETECTED] += 1\n",
    "    dictionary[(bbox_size,light_color_id)][INDEX_TOTAL_SAMPLE] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(graph_file):\n",
    "    \"\"\"Loads a frozen inference graph\"\"\"\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(graph_file, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "./model/site/ssd_mobilenet_v1_coco_10000_synthetic/frozen_inference_graph.pb; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fc30aa5fe9ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdetection_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/site/ssd_mobilenet_v1_coco_10000_synthetic/frozen_inference_graph.pb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fc30aa5fe9ec>\u001b[0m in \u001b[0;36mload_graph\u001b[0;34m(graph_file)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mod_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mserialized_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mod_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mod_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda3/envs/carnd-capstone/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    117\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda3/envs/carnd-capstone/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.pyc\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\n\u001b[0;32m---> 79\u001b[0;31m             compat.as_bytes(self.__name), 1024 * 512, status)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/miniconda3/envs/carnd-capstone/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ./model/site/ssd_mobilenet_v1_coco_10000_synthetic/frozen_inference_graph.pb; No such file or directory"
     ]
    }
   ],
   "source": [
    "\n",
    "def filter_boxes(min_score, boxes, scores, classes):\n",
    "    \"\"\"Return boxes with a confidence >= `min_score`\"\"\"\n",
    "    n = len(classes)\n",
    "    idxs = []\n",
    "    for i in range(n):\n",
    "        if scores[i] >= min_score:\n",
    "            idxs.append(i)\n",
    "    \n",
    "    filtered_boxes = boxes[idxs, ...]\n",
    "    filtered_scores = scores[idxs, ...]\n",
    "    filtered_classes = classes[idxs, ...]\n",
    "    return filtered_boxes, filtered_scores, filtered_classes\n",
    "\n",
    "def to_image_coords(boxes, height, width):\n",
    "    \"\"\"\n",
    "    The original box coordinate output is normalized, i.e [0, 1].\n",
    "    \n",
    "    This converts it back to the original coordinate based on the image\n",
    "    size.\n",
    "    \"\"\"\n",
    "    box_coords = np.zeros_like(boxes)\n",
    "    box_coords[:, 0] = boxes[:, 0] * height\n",
    "    box_coords[:, 1] = boxes[:, 1] * width\n",
    "    box_coords[:, 2] = boxes[:, 2] * height\n",
    "    box_coords[:, 3] = boxes[:, 3] * width\n",
    "    \n",
    "    return box_coords\n",
    "\n",
    "def draw_boxes(image, boxes, classes, colors, thickness=3):\n",
    "    \"\"\"Draw bounding boxes on the image\"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for i in range(len(boxes)):\n",
    "        bot, left, top, right = boxes[i, ...]\n",
    "        class_id = int(classes[i])\n",
    "        color = colors[class_id - 1]\n",
    "        draw.line([(left, top), (left, bot), (right, bot), (right, top), (left, top)], width=thickness, fill=color)\n",
    "\n",
    "\n",
    "detection_graph = load_graph('./model/site/ssd_mobilenet_v1_coco_10000_synthetic/frozen_inference_graph.pb')\n",
    "\n",
    "\n",
    "# The input placeholder for the image.\n",
    "# `get_tensor_by_name` returns the Tensor with the associated name in the Graph.\n",
    "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "# Each box represents a part of the image where a particular object was detected.\n",
    "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "# Each score represent how level of confidence for each of the objects.\n",
    "# Score is shown on the result image, together with the class label.\n",
    "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "\n",
    "# The classification of the object (integer id).\n",
    "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "\n",
    "\n",
    "image = Image.open('./evaluation-dataset/site/synthetic-4.jpg')\n",
    "image = Image.open('./assets/site-green-102.jpg')\n",
    "image = np.asarray(image)\n",
    "\n",
    "image = preprocess(image)\n",
    "# image = np.asarray(image)\n",
    "# image = adjust_gamma(image, 0.5)\n",
    "# image = Image.fromarray(image)\n",
    "\n",
    "image = Image.fromarray(image)\n",
    "image_np = np.expand_dims(np.asarray(image, dtype=np.uint8), 0)\n",
    "\n",
    "with tf.Session(graph=detection_graph) as sess:                \n",
    "    # Actual detection.\n",
    "    (boxes, scores, classes) = sess.run([detection_boxes, detection_scores, detection_classes], \n",
    "                                        feed_dict={image_tensor: image_np})\n",
    "\n",
    "    # Remove unnecessary dimensions\n",
    "    boxes = np.squeeze(boxes)\n",
    "    scores = np.squeeze(scores)\n",
    "    classes = np.squeeze(classes)\n",
    "\n",
    "    confidence_cutoff = 0.5\n",
    "    # Filter boxes with a confidence score less than `confidence_cutoff`\n",
    "    boxes, scores, classes = filter_boxes(confidence_cutoff, boxes, scores, classes)\n",
    "    print(scores)\n",
    "    print(classes)\n",
    "    # The current box coordinates are normalized to a range between 0 and 1.\n",
    "    # This converts the coordinates actual location on the image.\n",
    "    width, height = image.size\n",
    "    box_coords = to_image_coords(boxes, height, width)\n",
    "\n",
    "    # Each class with be represented by a differently colored box\n",
    "    draw_boxes(image, box_coords, classes, ['green', 'yellow', 'red', 'gray'])\n",
    "#     print(detection_classes)\n",
    "#     print(classes)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: it is possible that total expected lights is different from the detected correct plus detected incorrect\n",
      "since it is possible that we have two overlapping bounding boxes\n",
      "Green:\n",
      "Total expected lights: 968\n",
      "Detected Correct: 294\n",
      "Detected Incorrect: 2\n",
      "Not detected: 673\n",
      "Background as traffic light: 10\n",
      "Accuracy: 0.303719008264\n",
      "\n",
      "Red:\n",
      "Total expected lights: 1017\n",
      "Detected Correct: 56\n",
      "Detected Incorrect: 381\n",
      "Not detected: 590\n",
      "Background as traffic light: 2\n",
      "Accuracy: 0.055063913471\n",
      "\n",
      "Yellow:\n",
      "Total expected lights: 969\n",
      "Detected Correct: 6\n",
      "Detected Incorrect: 286\n",
      "Not detected: 683\n",
      "Background as traffic light: 0\n",
      "Accuracy: 0.0061919504644\n"
     ]
    }
   ],
   "source": [
    "#to store in the file where we record data for evaluation\n",
    "MODEL_NAME = \"ssd_mobilenet_v1_coco_40000_external_site\"\n",
    "NUM_STEPS = 40000\n",
    "DATABASE_TRAIN_NAME = \"test_train\"\n",
    "DATABASE_EVAL_NAME = \"SyntheticDataOriginalGamma\"\n",
    "SYNTHETIC_DATA = \"No\"\n",
    "\n",
    "evaluation_folder = './SyntheticDataOriginalGamma/'\n",
    "detection_graph = load_graph('./graphsModels/ssd_mobilenet_v1_coco_40000_external_site.pb')\n",
    "\n",
    "def distance(x1, y1, x2, y2):\n",
    "    return math.sqrt(pow(x2 - x1, 2) + pow(y2 - y1, 2))\n",
    "\n",
    "# The input placeholder for the image.\n",
    "# `get_tensor_by_name` returns the Tensor with the associated name in the Graph.\n",
    "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "# Each box represents a part of the image where a particular object was detected.\n",
    "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "# Each score represent how level of confidence for each of the objects.\n",
    "# Score is shown on the result image, together with the class label.\n",
    "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "\n",
    "# The classification of the object (integer id).\n",
    "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "\n",
    "label_mapping = {\n",
    "    'Green_light': 1,\n",
    "    'Red_light': 2,\n",
    "    'Yellow_light': 3\n",
    "}\n",
    "# image = preprocess(image)\n",
    "\n",
    "with tf.Session(graph=detection_graph) as sess:\n",
    "    # correct is when an expected bounding box is detected and is labelled correctly\n",
    "    correct = {\n",
    "        '1': 0,\n",
    "        '2': 0,\n",
    "        '3': 0,\n",
    "    }\n",
    "    # incorrect is when an expected bounding box is detected and is labelled incorrectly\n",
    "    incorrect = {\n",
    "        '1': 0,\n",
    "        '2': 0,\n",
    "        '3': 0,\n",
    "    }\n",
    "    # non-detected is when an expected bounding box is not detected\n",
    "    not_detected = {\n",
    "        '1': 0,\n",
    "        '2': 0,\n",
    "        '3': 0,\n",
    "    }\n",
    "    # background_as_trafficLight is when we have detected a bounding box but there is not any traffic light (i.e. when the bounding box detected falls into background)\n",
    "    background_as_trafficLight = {\n",
    "        '1': 0,\n",
    "        '2': 0,\n",
    "        '3': 0,\n",
    "    }\n",
    "    total = {\n",
    "        '1': 0,\n",
    "        '2': 0,\n",
    "        '3': 0,\n",
    "    }\n",
    "    \n",
    "    bbox_analysis = {}\n",
    "\n",
    "    for idx_image in range(500):\n",
    "        sample_name = 'synthetic-' + str(idx_image)\n",
    "        image = Image.open(evaluation_folder + sample_name + '.jpg')\n",
    "        image = np.asarray(image)\n",
    "        image = preprocess(image, 0.6)\n",
    "        image = Image.fromarray(image)\n",
    "        image_np = np.expand_dims(np.asarray(image, dtype=np.uint8), 0)\n",
    "        (boxes, scores, classes) = sess.run([detection_boxes, detection_scores, detection_classes], \n",
    "                                            feed_dict={image_tensor: image_np})\n",
    "\n",
    "        # Remove unnecessary dimensions\n",
    "        boxes = np.squeeze(boxes)\n",
    "        scores = np.squeeze(scores)\n",
    "        classes = np.squeeze(classes)\n",
    "\n",
    "        confidence_cutoff = 0.4\n",
    "        # Filter boxes with a confidence score less than `confidence_cutoff`\n",
    "        boxes, scores, classes = filter_boxes(confidence_cutoff, boxes, scores, classes)\n",
    "#         print(boxes)\n",
    "#         print(scores)\n",
    "        # The current box coordinates are normalized to a range between 0 and 1.\n",
    "        # This converts the coordinates actual location on the image.\n",
    "        width, height = image.size\n",
    "        box_coords = to_image_coords(boxes, height, width)\n",
    "        \n",
    "        label_xml = ET.parse(evaluation_folder + sample_name + '.xml')\n",
    "        expected_bbs = []\n",
    "        expected_classes = []\n",
    "        correpondent_detected = [False] * len(boxes)\n",
    "        for label in label_xml.findall('object'):\n",
    "            expected = label_mapping[label.find('name').text]\n",
    "            expected_classes.append(expected)\n",
    "#             print(expected)\n",
    "            box = label.find('bndbox')\n",
    "            min_x = int(box.find('xmin').text)\n",
    "            min_y = int(box.find('ymin').text)\n",
    "            max_x = int(box.find('xmax').text)\n",
    "            max_y = int(box.find('ymax').text)\n",
    "            \n",
    "            bbox_size = abs(max_y-min_y)*abs(max_x-min_x)\n",
    "            \n",
    "            expected_bbs.append([min_y, min_x, max_y, max_x])\n",
    "#             expected_bb = Polygon([\n",
    "#                 (min_y, min_x),\n",
    "#                 (max_x, min_y),\n",
    "#                 (max_x, max_y),\n",
    "#                 (min_x, max_y)\n",
    "#             ])\n",
    "            total[str(expected)] += 1\n",
    "#             print('\\ntesting')\n",
    "#             print([max_y, min_x, min_y, max_x])\n",
    "#             print('expected class: ' + str(expected))\n",
    "#             print('\\n')\n",
    "\n",
    "            detected = False\n",
    "            for i in range(len(box_coords)):\n",
    "                new_min_y, new_min_x, new_max_y, new_max_x = box_coords[i]\n",
    "#                 actual_bb = Polygon([\n",
    "#                     (new_min_x, new_min_y),\n",
    "#                     (new_max_x, new_min_y),\n",
    "#                     (new_max_x, new_max_y),\n",
    "#                     (new_min_x, new_max_y)\n",
    "#                 ])\n",
    "#                 intersection = expected_bb.intersection(actual_bb)\n",
    "#                 area_of_overlap = intersection.area / actual_bb.area if actual_bb.area < expected_bb.area else intersection.area / expected_bb.area\n",
    "#                 print(box_coords[i])\n",
    "#                 print('overlap: ' + str(area_of_overlap))\n",
    "                # consider it a match if there is 75% overlap\n",
    "                # distance of top left corners\n",
    "                dist_tl = distance(min_x, max_y, new_min_x, new_min_y)\n",
    "#                 print(dist_tl)\n",
    "                # distance of bottom right corners\n",
    "                dist_br = distance(max_x, min_y, new_max_x, new_max_y)\n",
    "#                 print(dist_br)\n",
    "#                 print('predicted class: ' + str(int(classes[i])))\n",
    "                avg_corner_distance = (dist_tl + dist_br) / 2\n",
    "#                 print('avg corner distance: ' + str(avg_corner_distance))\n",
    "                if avg_corner_distance < 40:# and area_of_overlap > 0.7:\n",
    "                    correpondent_detected[i] = True\n",
    "                    detected = True\n",
    "                    if int(classes[i]) == expected:\n",
    "                        correct[str(expected)] += 1\n",
    "                        add_bbox_sample(bbox_analysis, bbox_size, str(expected), True, True)\n",
    "                    else:\n",
    "                        incorrect[str(expected)] += 1\n",
    "                        add_bbox_sample(bbox_analysis, bbox_size, str(expected), True, False)\n",
    "            if not detected:\n",
    "                not_detected[str(expected)] += 1\n",
    "                add_bbox_sample(bbox_analysis, bbox_size, str(expected), False, False)\n",
    "        \n",
    "        for i in range(len(box_coords)):\n",
    "            if correpondent_detected[i] is False:\n",
    "                background_as_trafficLight[str(int(classes[i]))] += 1\n",
    "            \n",
    "            #d = dict(zip(unique, counts))\n",
    "            #non_correspondance[int(classes[i])] += d[False]\n",
    "\n",
    "#         print(expected_bbs)\n",
    "\n",
    "#         print(np.array(expected_bbs))\n",
    "#         print(box_coords)\n",
    "#         draw_boxes(image, np.array(expected_bbs), expected_classes, ['#c2f6a8', '#f68d8d', '#f6eaa8', 'gray'])\n",
    "#         draw_boxes(image, box_coords, classes, ['green', 'red', 'yellow', 'gray'])\n",
    "    #     print(detection_classes)\n",
    "    #     print(classes)\n",
    "\n",
    "#         plt.figure(figsize=(12, 8))\n",
    "#         plt.imshow(image) \n",
    "    print('Note: it is possible that total expected lights is different from the detected correct plus detected incorrect\\nsince it is possible that we have two overlapping bounding boxes')\n",
    "    print('Green:')\n",
    "    print('Total expected lights: ' + str(total['1']))\n",
    "    print('Detected Correct: ' + str(correct['1']))\n",
    "    print('Detected Incorrect: ' + str(incorrect['1']))\n",
    "    print('Not detected: ' + str(not_detected['1']))\n",
    "    print('Background as traffic light: ' + str(background_as_trafficLight['1']))\n",
    "    print('Accuracy: '+  str(correct['1'] / float(total['1'])))\n",
    "    print('\\nRed:')\n",
    "    print('Total expected lights: ' + str(total['2']))\n",
    "    print('Detected Correct: ' + str(correct['2']))\n",
    "    print('Detected Incorrect: ' + str(incorrect['2']))\n",
    "    print('Not detected: ' + str(not_detected['2']))\n",
    "    print('Background as traffic light: ' + str(background_as_trafficLight['2']))\n",
    "    print('Accuracy: '+  str(correct['2'] / float(total['2'])))\n",
    "    print('\\nYellow:')\n",
    "    print('Total expected lights: ' + str(total['3']))\n",
    "    print('Detected Correct: ' + str(correct['3']))\n",
    "    print('Detected Incorrect: ' + str(incorrect['3']))\n",
    "    print('Not detected: ' + str(not_detected['3']))\n",
    "    print('Background as traffic light: ' + str(background_as_trafficLight['3']))\n",
    "    print('Accuracy: '+  str(correct['3'] / float(total['3'])))\n",
    "    \n",
    "\n",
    "    with open(\"evaluation_model_accuracy_site_11nov.csv\", \"a\") as myfile:\n",
    "        for i in range(3): # for all the colors\n",
    "            myfile.write(MODEL_NAME + \",\")\n",
    "            myfile.write(str(NUM_STEPS) + \",\")\n",
    "            myfile.write(DATABASE_TRAIN_NAME + \",\")\n",
    "            myfile.write(DATABASE_EVAL_NAME + \",\")\n",
    "            myfile.write(SYNTHETIC_DATA + \",\")\n",
    "            myfile.write(str(i+1) + \",\") #colour\n",
    "            myfile.write(str(total[str(i+1)]) + \", \")\n",
    "            myfile.write(str(correct[str(i+1)]) + \", \")\n",
    "            myfile.write(str(incorrect[str(i+1)]) + \", \")\n",
    "            myfile.write(str(not_detected[str(i+1)]) + \", \")\n",
    "            myfile.write(str(background_as_trafficLight[str(i+1)]))\n",
    "            myfile.write(\"\\n\")\n",
    "\n",
    "\n",
    "    with open(\"evaluation_model_accuracy_bbox_site_11nov.csv\", \"a\") as myfile:\n",
    "        for x in bbox_analysis.keys():\n",
    "            myfile.write(MODEL_NAME + \",\")\n",
    "            myfile.write(str(NUM_STEPS) + \",\")\n",
    "            myfile.write(DATABASE_TRAIN_NAME + \",\")\n",
    "            myfile.write(DATABASE_EVAL_NAME + \",\")\n",
    "            myfile.write(str(x[1]) + \",\") # traffic light color\n",
    "            myfile.write(str(x[0]) + \",\") # bbox size\n",
    "            myfile.write(str(bbox_analysis[x][INDEX_DETECT_CORRECTLY]) + \",\") # detection correct\n",
    "            myfile.write(str(bbox_analysis[x][INDEX_DETECT_INCORRECTLY]) + \",\") # detection incorrect\n",
    "            myfile.write(str(bbox_analysis[x][INDEX_NOT_DETECTED]) + \",\") # not detected\n",
    "            myfile.write(str(bbox_analysis[x][INDEX_TOTAL_SAMPLE])) # total samples\n",
    "            myfile.write(\"\\n\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis with different gamma steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to store in the file where we record data for evaluation\n",
    "MODEL_NAME = \"ssd_mobilenet_v1_coco_10000_gamma\"\n",
    "NUM_STEPS = 10000\n",
    "DATABASE_TRAIN_NAME = \"test_train\"\n",
    "DATABASE_EVAL_NAME = \"SyntheticDataOriginalGamma\"\n",
    "SYNTHETIC_DATA = \"No\"\n",
    "\n",
    "evaluation_folder = './SyntheticDataOriginalGamma/'\n",
    "detection_graph = load_graph('./graphsModels/ssd_mobilenet_v1_coco_10000_gamma.pb')\n",
    "\n",
    "def distance(x1, y1, x2, y2):\n",
    "    return math.sqrt(pow(x2 - x1, 2) + pow(y2 - y1, 2))\n",
    "\n",
    "# The input placeholder for the image.\n",
    "# `get_tensor_by_name` returns the Tensor with the associated name in the Graph.\n",
    "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "# Each box represents a part of the image where a particular object was detected.\n",
    "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "# Each score represent how level of confidence for each of the objects.\n",
    "# Score is shown on the result image, together with the class label.\n",
    "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "\n",
    "# The classification of the object (integer id).\n",
    "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "\n",
    "label_mapping = {\n",
    "    'Green_light': 1,\n",
    "    'Red_light': 2,\n",
    "    'Yellow_light': 3\n",
    "}\n",
    "# image = preprocess(image)\n",
    "\n",
    "with tf.Session(graph=detection_graph) as sess:\n",
    "    for i in xrange(40,100,5):\n",
    "        # correct is when an expected bounding box is detected and is labelled correctly\n",
    "        correct = {\n",
    "            '1': 0,\n",
    "            '2': 0,\n",
    "            '3': 0,\n",
    "        }\n",
    "        # incorrect is when an expected bounding box is detected and is labelled incorrectly\n",
    "        incorrect = {\n",
    "            '1': 0,\n",
    "            '2': 0,\n",
    "            '3': 0,\n",
    "        }\n",
    "        # non-detected is when an expected bounding box is not detected\n",
    "        not_detected = {\n",
    "            '1': 0,\n",
    "            '2': 0,\n",
    "            '3': 0,\n",
    "        }\n",
    "        # background_as_trafficLight is when we have detected a bounding box but there is not any traffic light (i.e. when the bounding box detected falls into background)\n",
    "        background_as_trafficLight = {\n",
    "            '1': 0,\n",
    "            '2': 0,\n",
    "            '3': 0,\n",
    "        }\n",
    "        total = {\n",
    "            '1': 0,\n",
    "            '2': 0,\n",
    "            '3': 0,\n",
    "        }\n",
    "\n",
    "        bbox_analysis = {}\n",
    "        gammaValue = i/100.\n",
    "    \n",
    "        for idx_image in range(500):\n",
    "            sample_name = 'synthetic-' + str(idx_image)\n",
    "            image = Image.open(evaluation_folder + sample_name + '.jpg')\n",
    "            image = np.asarray(image)\n",
    "            image = preprocess(image, gammaValue)\n",
    "            image = Image.fromarray(image)\n",
    "            image_np = np.expand_dims(np.asarray(image, dtype=np.uint8), 0)\n",
    "            (boxes, scores, classes) = sess.run([detection_boxes, detection_scores, detection_classes], \n",
    "                                                feed_dict={image_tensor: image_np})\n",
    "\n",
    "            # Remove unnecessary dimensions\n",
    "            boxes = np.squeeze(boxes)\n",
    "            scores = np.squeeze(scores)\n",
    "            classes = np.squeeze(classes)\n",
    "\n",
    "            confidence_cutoff = 0.4\n",
    "            # Filter boxes with a confidence score less than `confidence_cutoff`\n",
    "            boxes, scores, classes = filter_boxes(confidence_cutoff, boxes, scores, classes)\n",
    "    #         print(boxes)\n",
    "    #         print(scores)\n",
    "            # The current box coordinates are normalized to a range between 0 and 1.\n",
    "            # This converts the coordinates actual location on the image.\n",
    "            width, height = image.size\n",
    "            box_coords = to_image_coords(boxes, height, width)\n",
    "\n",
    "            label_xml = ET.parse(evaluation_folder + sample_name + '.xml')\n",
    "            expected_bbs = []\n",
    "            expected_classes = []\n",
    "            correpondent_detected = [False] * len(boxes)\n",
    "            for label in label_xml.findall('object'):\n",
    "                expected = label_mapping[label.find('name').text]\n",
    "                expected_classes.append(expected)\n",
    "    #             print(expected)\n",
    "                box = label.find('bndbox')\n",
    "                min_x = int(box.find('xmin').text)\n",
    "                min_y = int(box.find('ymin').text)\n",
    "                max_x = int(box.find('xmax').text)\n",
    "                max_y = int(box.find('ymax').text)\n",
    "\n",
    "                bbox_size = abs(max_y-min_y)*abs(max_x-min_x)\n",
    "\n",
    "                expected_bbs.append([min_y, min_x, max_y, max_x])\n",
    "    #             expected_bb = Polygon([\n",
    "    #                 (min_y, min_x),\n",
    "    #                 (max_x, min_y),\n",
    "    #                 (max_x, max_y),\n",
    "    #                 (min_x, max_y)\n",
    "    #             ])\n",
    "                total[str(expected)] += 1\n",
    "    #             print('\\ntesting')\n",
    "    #             print([max_y, min_x, min_y, max_x])\n",
    "    #             print('expected class: ' + str(expected))\n",
    "    #             print('\\n')\n",
    "\n",
    "                detected = False\n",
    "                for i in range(len(box_coords)):\n",
    "                    new_min_y, new_min_x, new_max_y, new_max_x = box_coords[i]\n",
    "    #                 actual_bb = Polygon([\n",
    "    #                     (new_min_x, new_min_y),\n",
    "    #                     (new_max_x, new_min_y),\n",
    "    #                     (new_max_x, new_max_y),\n",
    "    #                     (new_min_x, new_max_y)\n",
    "    #                 ])\n",
    "    #                 intersection = expected_bb.intersection(actual_bb)\n",
    "    #                 area_of_overlap = intersection.area / actual_bb.area if actual_bb.area < expected_bb.area else intersection.area / expected_bb.area\n",
    "    #                 print(box_coords[i])\n",
    "    #                 print('overlap: ' + str(area_of_overlap))\n",
    "                    # consider it a match if there is 75% overlap\n",
    "                    # distance of top left corners\n",
    "                    dist_tl = distance(min_x, max_y, new_min_x, new_min_y)\n",
    "    #                 print(dist_tl)\n",
    "                    # distance of bottom right corners\n",
    "                    dist_br = distance(max_x, min_y, new_max_x, new_max_y)\n",
    "    #                 print(dist_br)\n",
    "    #                 print('predicted class: ' + str(int(classes[i])))\n",
    "                    avg_corner_distance = (dist_tl + dist_br) / 2\n",
    "    #                 print('avg corner distance: ' + str(avg_corner_distance))\n",
    "                    if avg_corner_distance < 40:# and area_of_overlap > 0.7:\n",
    "                        correpondent_detected[i] = True\n",
    "                        detected = True\n",
    "                        if int(classes[i]) == expected:\n",
    "                            correct[str(expected)] += 1\n",
    "                            add_bbox_sample(bbox_analysis, bbox_size, str(expected), True, True)\n",
    "                        else:\n",
    "                            incorrect[str(expected)] += 1\n",
    "                            add_bbox_sample(bbox_analysis, bbox_size, str(expected), True, False)\n",
    "                if not detected:\n",
    "                    not_detected[str(expected)] += 1\n",
    "                    add_bbox_sample(bbox_analysis, bbox_size, str(expected), False, False)\n",
    "\n",
    "            for i in range(len(box_coords)):\n",
    "                if correpondent_detected[i] is False:\n",
    "                    background_as_trafficLight[str(int(classes[i]))] += 1\n",
    "\n",
    "                #d = dict(zip(unique, counts))\n",
    "                #non_correspondance[int(classes[i])] += d[False]\n",
    "\n",
    "    #         print(expected_bbs)\n",
    "\n",
    "    #         print(np.array(expected_bbs))\n",
    "    #         print(box_coords)\n",
    "    #         draw_boxes(image, np.array(expected_bbs), expected_classes, ['#c2f6a8', '#f68d8d', '#f6eaa8', 'gray'])\n",
    "    #         draw_boxes(image, box_coords, classes, ['green', 'red', 'yellow', 'gray'])\n",
    "        #     print(detection_classes)\n",
    "        #     print(classes)\n",
    "\n",
    "    #         plt.figure(figsize=(12, 8))\n",
    "    #         plt.imshow(image) \n",
    "   \n",
    "    \n",
    "\n",
    "        with open(\"evaluation_model_accuracy_gamma_11nov.csv\", \"a\") as myfile:\n",
    "            for i in range(3): # for all the colors\n",
    "                myfile.write(MODEL_NAME + \",\")\n",
    "                myfile.write(str(NUM_STEPS) + \",\")\n",
    "                myfile.write(DATABASE_TRAIN_NAME + \",\")\n",
    "                myfile.write(DATABASE_EVAL_NAME + \",\")\n",
    "                myfile.write(SYNTHETIC_DATA + \",\")\n",
    "                myfile.write(str(i+1) + \",\") #colour\n",
    "                myfile.write(str(total[str(i+1)]) + \", \")\n",
    "                myfile.write(str(correct[str(i+1)]) + \", \")\n",
    "                myfile.write(str(incorrect[str(i+1)]) + \", \")\n",
    "                myfile.write(str(not_detected[str(i+1)]) + \", \")\n",
    "                myfile.write(str(background_as_trafficLight[str(i+1)]) + \", \")\n",
    "                myfile.write(str(gammaValue))\n",
    "                myfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing Detection\n",
    "\n",
    "The model zoo comes with a variety of models, each its benefits and costs. Below you'll time some of these models. The general tradeoff being sacrificing model accuracy for seconds per frame (SPF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_detection(sess, runs=10):\n",
    "    image_tensor = sess.graph.get_tensor_by_name('image_tensor:0')\n",
    "    detection_boxes = sess.graph.get_tensor_by_name('detection_boxes:0')\n",
    "    detection_scores = sess.graph.get_tensor_by_name('detection_scores:0')\n",
    "    detection_classes = sess.graph.get_tensor_by_name('detection_classes:0')\n",
    "    \n",
    "    numImages = 100\n",
    "    times = []\n",
    "    for idx_image in range(numImages):\n",
    "        sample_name = 'synthetic-' + str(idx_image)\n",
    "        image = Image.open(evaluation_folder + sample_name + '.jpg')\n",
    "        image_np = np.expand_dims(np.asarray(image, dtype=np.uint8), 0)\n",
    "        \n",
    "        for i in range(runs):\n",
    "            t0 = time.time()\n",
    "            sess.run([detection_boxes, detection_scores, detection_classes], feed_dict={image_tensor: image_np})\n",
    "            t1 = time.time()\n",
    "            times.append((t1 - t0) * 1000)\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to store in the file where we record data for evaluation\n",
    "MODEL_NAME = \"ssd_mobilenet_v1_coco_10000\"        \n",
    "evaluation_folder = './synthetic-dataset-oct-30-with-labels/'\n",
    "detection_graph = load_graph('./graphsModels/ssd_mobilenet_v1_coco_10000.pb')\n",
    "\n",
    "with tf.Session(graph=detection_graph) as sess:\n",
    "    times = time_detection(sess, runs=10)\n",
    "    with open(\"evaluation_model_object_detection_timings.csv\", \"a\") as myfile:\n",
    "        myfile.write(MODEL_NAME + \",\") \n",
    "        myfile.write(str(np.mean(times)) + \",\") \n",
    "        myfile.write(str(np.std(times)))\n",
    "        myfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure instance\n",
    "fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "# Create an axes instance\n",
    "ax = fig.add_subplot(111)\n",
    "plt.title(\"Object Detection Timings\")\n",
    "plt.ylabel(\"Time (ms)\")\n",
    "\n",
    "# Create the boxplot\n",
    "plt.style.use('fivethirtyeight')\n",
    "bp = ax.boxplot(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161.2577597816605"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(times)\n",
    "np.std(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Model Tradeoffs\n",
    "\n",
    "Download a few models from the [model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) and compare the timings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection on a Video\n",
    "\n",
    "Finally run your pipeline on [this short video](https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/advanced_deep_learning/driving.mp4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"600\" controls>\n",
       "  <source src=\"driving.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"600\" controls>\n",
    "  <source src=\"{0}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format('driving.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 5 - Object Detection on a Video\n",
    "\n",
    "Run an object detection pipeline on the above clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = VideoFileClip('driving.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this function.\n",
    "# The input is an NumPy array.\n",
    "# The output should also be a NumPy array.\n",
    "def pipeline(img):\n",
    "    draw_img = Image.fromarray(img)\n",
    "    # Actual detection.\n",
    "    (boxes, scores, classes) = sess.run([detection_boxes, detection_scores, detection_classes], \n",
    "                                        feed_dict={image_tensor: np.expand_dims(img, 0)})\n",
    "\n",
    "    # Remove unnecessary dimensions\n",
    "    boxes = np.squeeze(boxes)\n",
    "    scores = np.squeeze(scores)\n",
    "    classes = np.squeeze(classes)\n",
    "\n",
    "    confidence_cutoff = 0.3\n",
    "    # Filter boxes with a confidence score less than `confidence_cutoff`\n",
    "    boxes, scores, classes = filter_boxes(confidence_cutoff, boxes, scores, classes)\n",
    "\n",
    "    # The current box coordinates are normalized to a range between 0 and 1.\n",
    "    # This converts the coordinates actual location on the image.\n",
    "    width, height = draw_img.size\n",
    "    box_coords = to_image_coords(boxes, height, width)\n",
    "\n",
    "    # Each class with be represented by a differently colored box\n",
    "    draw_boxes(draw_img, box_coords, classes)\n",
    "    return np.array(draw_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Sample solution](./exercise-solutions/e5.py)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video result.mp4\n",
      "[MoviePy] Writing video result.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1782/1782 [07:31<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: result.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=detection_graph) as sess:\n",
    "    image_tensor = sess.graph.get_tensor_by_name('image_tensor:0')\n",
    "    detection_boxes = sess.graph.get_tensor_by_name('detection_boxes:0')\n",
    "    detection_scores = sess.graph.get_tensor_by_name('detection_scores:0')\n",
    "    detection_classes = sess.graph.get_tensor_by_name('detection_classes:0')\n",
    "    \n",
    "    new_clip = clip.fl_image(pipeline)\n",
    "    \n",
    "    # write to file\n",
    "    new_clip.write_videofile('result.mp4', audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"600\" controls>\n",
       "  <source src=\"result.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"600\" controls>\n",
    "  <source src=\"{0}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format('result.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Exploration\n",
    "\n",
    "Some ideas to take things further:\n",
    "\n",
    "* Finetune the model on a new dataset more relevant to autonomous vehicles. Instead of loading the frozen inference graph you'll load the checkpoint.\n",
    "* Optimize the model and get the FPS as low as possible.\n",
    "* Build your own detector. There are several base model pretrained on ImageNet you can choose from. [Keras](https://keras.io/applications/) is probably the quickest way to get setup in this regard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
